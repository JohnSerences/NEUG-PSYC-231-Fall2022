{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InClass exercises (randomization and bootstrapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First start by importing the packages you'll need. \n",
    "* Numpy, scipy, and matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats  # has t-tests and other stats stuff...\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Start with some data. This is a slightly modified part of the Anscombe data set\n",
    "* First plot it\n",
    "* Then compute the correlation coeffecient that relates the two arrays\n",
    "* Then the t-value and p-value associated with the correlation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = np.array([8.1, 8.01, 7.8, 7.85, 8.21, 8.11, 7.95, 12.5, 8.05, 7.98, 8.13])\n",
    "d2 = np.array([6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 19, 5.56, 7.91, 6.89])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok - now scatterplot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on this plot, what do you think is going to happen when we compare the output from a parametric test and a radomization test?\n",
    "* First compute parametric correlation coef and p-value..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametric correlation coeffecient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now use randomization testing to put a (slightly) better p-value on the data...\n",
    "* Run 10000 iterations of the randomization test to evalute how likely your correltation value is under the assumption that your condition label makes no difference (ie. that your manipulation is meaningless)\n",
    "* On each iteration randomly decide if each data point is assigned to one condition or the other\n",
    "    * One easy way to do this is to flip a coin to determine assignment of each data point (e.g. `if np.random.rand() < .5` then assign a data point to condition 1, otherwise to condition 2\n",
    "* then re-compute correlation on each iteration\n",
    "* then compare the distribution of p values under the null to your obtained p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Would you still want to publish these data? Why does label swapping here still result in such a low p-value when the correlation is visibly so dubious? Why would bootstrapping be more appropriate here? Give it a try to see what happens...\n",
    "* Resample the data **with replacement** (10000 times or whatever)\n",
    "* On each bootstrap iteration recompute your correlation\n",
    "* Then compute confidence intervals...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
