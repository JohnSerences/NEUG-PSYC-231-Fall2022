{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In class exercise...Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mutual_info_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the eeg data from previous tutorials...\n",
    "* `data` is a trials x time matrix of EEG responses\n",
    "* `sr` is the sample rate\n",
    "* `tx` is the time associated with each sample on each trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data...\n",
    "eeg = np.load('eeg_data01.npz')\n",
    "\n",
    "# get the different arrays like this...kind of like a structure in matlab. \n",
    "eeg['data']\n",
    "eeg['sr']\n",
    "eeg['tx']\n",
    "\n",
    "# and can query the attributes of the data like this...which will tell us that there 1600 trials and 4102 timepoints per trial sampled\n",
    "# at 1024Hz\n",
    "print('Shape of the big eeg data set: ', eeg['data'].shape)\n",
    "print('Sample rate: ', eeg['sr'])\n",
    "\n",
    "# and if you want to save some typing, especially because we only have a few variables, you reassign the different arrays like this\n",
    "data = eeg['data']\n",
    "data[400:800,:] += 10\n",
    "sr = eeg['sr']\n",
    "tx = eeg['tx']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a vector that labels each trial as coming from experimental conditions 1,2,3,4\n",
    "* Figure out how many trials there are in the data set (let's call that `N`). \n",
    "* Divide that by 4\n",
    "* then make a vector (call it `cond`) that has N/4 1s, followed by N/4 2s, followed by N/4 3s, followed by N/4 4s (I would suggest using `np.hstack` and `np.ones` to achieve this but there are many ways...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now avgerage the data from condition 2, avg the data from condition 3, and plot against the time axis (tx) - we're going to ignore conditions 1 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then compute the MI between evoked responses and experimental condition in a specific time window from 1000ms to 1500ms post-stimulus onset (so >= 1000 and <1500)\n",
    "* You have one discrete variable (cond, or experimental condition label)\n",
    "* You have the EEG data from each condition, which is a continuous variable\n",
    "* First compute the mean response across the time window for each trial of condition 2. This should give you a 400 element array with the mean from each trial across the time window. \n",
    "* Then compute the mean response across the time window for each trial of condition 3. \n",
    "* Then concatenate the two 400 element arrays to make one 800 element array\n",
    "* You might also want to create a new set of labels for each trial in your new concatenated array - can relabel conds 2 and 3 with a 0 and 1 for ease of looping...so it would have 400 0's followed by 400 1's\n",
    "* Use only 10 bins for discretizing the continuous EEG data...\n",
    "* Then compute the MI between experimental condition and EEG responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### first compute means over time window for conds 2 and 3, stack up..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### then make new labels...400 zeros stacked on top of 400 ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### now compute MI!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does that number seem kinda low given the visual separation of the mean responses you plotted above? Plot some histograms of the data from condition 2 vs condition 3...how much overlap is there? Does the MI value you computed now seem more reasonable? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge question: How \"good\" is that MI value? What is the max value it could take in this scenario? How might you compute a `normalized` MI metric that tells you how much information there is relative to that max value? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
