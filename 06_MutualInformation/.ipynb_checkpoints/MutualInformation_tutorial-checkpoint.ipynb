{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy, conditional entropy, and mutual information\n",
    "\n",
    "* js (jserences@ucsd.edu), Fall 2022 for NEUG-PSYC-231\n",
    "\n",
    "Overview:\n",
    "* In this tutorial, we will learn how to characterize the information that is shared between two variables (mutual information). In other words, how much is uncertainty reduced about variable 1 after we've measured variable 2? \n",
    "\n",
    "* These concepts were initially developed in communication theory to describe the efficacy of transmitting signals over a noisy medium. For example, suppose that we want to know how good a communication channel is, or its effeciency in reliably relaying a message from point A (a 'sender') to point B (a 'reciever').\n",
    "\n",
    "* Basically this is just like asking, \"we know how good the signal is at A, and we recieved the message at B - how much information about A is still in the received signal B?\". \n",
    "\n",
    "* Mutual information is also super relevant in many fields of neuroscience, psychology, engeneering, etc (and there are much fancier multivariate versions that we won't cover here). For example, in neuroscience we're dealing with a series of communication channels that are corrupted by noise (e.g. info relayed via synapses). We can then ask all kinds of questions like how much information from neuron A effectively propogates to neuron B? Or does attention orb learning increase the mutual information between neurons? \n",
    "\n",
    "* Importantly, we can apply these tools to any combination of variables: two continuous variables, two discrete variables, one continuous and one discrete, etc. \n",
    "\n",
    "* Note: this also comes up in machine learning sometimes, where you can use mutual information to select the most informative features in a training data set (sklearn has some great tools for this)\n",
    "\n",
    "* A few notes before we get started. First, we're going to be talking a lot about **uncertainty** and **uncertainty reduction**. While this is basically complementary to talking about certainty and an increase in certainty, we'll deal with the former terminology as it is embedded in some of the concepts that we'll discuss. \n",
    "\n",
    "* Second, we'll be dealing with variability in data, and how we can either attribute that variability in the data to 'noise' or to 'signals'. I.e. does the variability in one variable systematically change with the variability in another? \n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "A lot of people think, after first hearing about mutual information, \"why not just correlate the variables using a normal r-value?\". There are a few answers to this, but the simplest is this: correlation assumes a linear relationship (or, in more complex forms, you have to assume some relationship) between variables. Mutual information does not, and can generally capture any form of linear or non-linear relationship between two variables. This makes it a very powerful and general purpose metric.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# also define the default font we'll use for figures.\n",
    "font = {'family' : 'DejaVu Sans',\n",
    "        'weight' : 'normal',\n",
    "        'size'   : 20}\n",
    "\n",
    "# then set the figure attributes\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART II: Entropy as a measure of variability\n",
    "\n",
    "* Shannon Entropy is related to the variability of data, but is more specifically defined as the average uncertainty in a set of measurements \n",
    "* Consider coin tosses - lets say we have a balanced coin, and we flip it once. We can represent the outcome of a single toss as a 0 or a 1 (a head or a tail), and this has an entropy (uncertainty) of 1 bit. In other words, we would reduce our uncertainty completely after we observed the outcome, and that would correspond to a reduction in uncertainty of 1 bit. You could alternatively say that we learned 1 bit of information by making our measurement.  \n",
    "    * By extension, if we flipped the coin twice, then the entropy would be 2 bits (00, 01, 10, or 11) because we would reduce our uncertainty by 2 bits after we observed the outcome. \n",
    "\n",
    "* To keep with the coin analogy...when the coin is balanced and heads and tails are equally likely, then the entropy is highest because uncertainty is maximized and flipping the coin will give you 1 bit of information.  \n",
    "* To see why entropy is maximized in this situation, consider a biased coin  that comes up heads 60% the time. \n",
    "    * In this case, we could predict the outcome of the coin flip better than chance simply by going with our prior of 'heads'. Thus, the entropy assoicated with a biased coin is less than the entropy associated with the unbiased coin, because the reduction in uncertainty is lower with the biased coin than with the unbaised coin. \n",
    "    * Put another way: we learn less after flipping the biased coin than we do when we flip the unbiased coin. \n",
    "* This example brings up an important point: entropy as a measure of uncertainty is maximized when all possible outcomes are equally likely because you have no prior info upon which to make an educated guess about the outcome (i.e. a uniform distribution over the span of possible outcomes has the highest entropy). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure out the entropy of a measurement in bits (the most common metric)\n",
    "* We can use the log2 function, referred to as the binary logarithm and the inverse function of 2^N. \n",
    "* The log2(n) is the power to which the number 2 must be raised to obtain the value n. \n",
    "* Lets go back to our coin flip example with a fair coin. Suppose you flipped the coin once - the entropy would be 1 bit because there are two possible outcomes...i.e., you can represent the outcome with 1 bit, or (0,1)\n",
    "*  If you flipped the coin twice you'd have 4 possible outcomes (00,10,01,11), or three times you'd have 8 possible outcomes (000, 001...etc). So you'd need 2 and 3 bits, respectively, to encode all possible outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.log2(2)) # log2(n) or 2^x = n? ...x = 1\n",
    "print(np.log2(4)) # or 2^x = n, x = 2\n",
    "print(np.log2(8)) # or 2^x = n, x = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To see the general shape of the function, plot out log2(x:y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# why am i going from 1,15 instead of our usual 0 based counting?\n",
    "plt.plot(np.arange(1,15), np.log2(np.arange(1,15)), linewidth=3)\n",
    "plt.ylim([-1,5])\n",
    "plt.ylabel('Entropy (2^N = ?)')\n",
    "plt.xlabel('# of possible outcomes')\n",
    "plt.show()\n",
    "# note that log2(0) == -inf, and that log2(1)==0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going back to the above notion that entropy is maximized when the outcome is completely uncertain (e.g. a fair coin), then we can start to develop an intuition about expressing entropy in terms of the probability that some event will occur, denoted `p(x)`. \n",
    "* lets say that we have a slot machine that has one wheel on it and the wheel can take one of N states. \n",
    "* Considering just the first wheel, the possible outcomes are {x1....xn}, and if each outcome is equally likely, then p(xi)= 1/n.  \n",
    "* So, for example, if n = 16, then the total entropy of the wheel can be represented  by 4 bits of information (or there is 4 bits of info to be gained by spinning the wheel and observing the outcome). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 16\n",
    "-np.log2(1/n)  \n",
    "#why negative when dealing with probabilities? log2(1)-log2(n), log2(1) = 0.0, so 0-log2(n) is neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we'd like though is a way to assess the *average* uncertainty of a particular outcome across all possible outcomes. How would you do that? \n",
    "* You'd take the uncertainty of each outcome (-log2(p(xi)) and weight it by the probability that the event will actually  occur, like this (where entropy is denoted, by convention, as H):\n",
    "\n",
    "`H = -( (1/n) * np.log2(1/n) )` where `n` is the number of possible outcomes\n",
    "\n",
    "* Note that the \"average\" uncertainty is achieved by weighting each with the probability that the event occurs (see the commented out code below for another way of doing this that might make more sense...but use the formula in the uncommented code when actually computing). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "# H = -sum_over_all_i( p(xi) * log2(p(xi)) )\n",
    "n = 16\n",
    "H = 0\n",
    "\n",
    "for i in np.arange(n):\n",
    "    \n",
    "    H += -( (1/n) * np.log2(1/n) )\n",
    "\n",
    "print(H)\n",
    "\n",
    "# note: in this case of equal probability outcomes\n",
    "# you could also do this:\n",
    "# H = 0\n",
    "# for i in np.arange(n):\n",
    "#     H += -np.log2(1/n)\n",
    "\n",
    "# print(H/n)  # just normalize here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about when all events are not equally likely?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = np.arange(.001,.999,.001)\n",
    "\n",
    "px = np.zeros( ( 2, len(probs) ) )\n",
    "\n",
    "px[0,:] = probs    # p(heads)\n",
    "px[1,:] = 1 - probs   # p(tails)\n",
    "\n",
    "# entropy \n",
    "e_heads = ( px[0,:] * np.log2(px[0,:]) ) * -1\n",
    "e_tails = ( px[1,:] * np.log2(px[1,:]) ) * -1\n",
    "H = e_heads + e_tails\n",
    "\n",
    "plt.plot(probs, H, 'b', linewidth=2)\n",
    "plt.xlabel('Probability of heads (coin bias)')\n",
    "plt.ylabel('entropy (bits)')\n",
    "plt.axvline(.5, color='k', linewidth=2)\n",
    "plt.axhline(1, color='k', linewidth=2)\n",
    "plt.show()\n",
    "# entropy is maximized with maximum uncertainty and will max out at\n",
    "# log2(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUMMARY UP TILL NOW\n",
    "* Entropy is a measure of uncertainty, and as uncertainty goes up (and is maximized when all outcomes are equally likely) then the information gained by making a measurement goes up. \n",
    "* So if you know the oucome in advance (e.g. a coin with two heads) then p(tails) = 0 and there is no uncertainty, entropy is 0, and there is no reduction in uncertainty to be gained by flipping the coin at all. \n",
    "* If you have a fair coin, then p(head)==p(tail) and entropy will be maximum and you will maximally reduce your uncertainty by making the measurement (in this case, you will fully disambiguate the outcome, gaining 1 bit of information where the total uncertainty is 1 bit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mutual information (MI). \n",
    "\n",
    "* MI is a measure of how much knowing  about 1 variable tells you about the state of another variable. Putting aside entropy and measures of uncertainty/variance for a minute, here is the  intuition. \n",
    "    * Suppose you have two variables that are completely unrelated to each other: measuring one variable will tell you nothing about the state of the other variable. \n",
    "    * In contrast, if you have two variables that are perfectly related, then measuring one variable will tell you everything about the state of the other. \n",
    "    * In this special (unusual) case, the mutual information will be equal to the entropy of either variable alone (that is: the information gained by measuring one variable will be equal to the information gained by measuring either - i.e., if they are perfectly related then you only need to measure one of them to know everything about the other)\n",
    " \n",
    "To put this back in terms of entropy: lets say we have two variables, X and Y. If we want to assess the MI between X and Y, then we need to know the following difference score:\n",
    "\n",
    "`(total entropy of X) - (entropy of X given Y)` \n",
    "\n",
    "In other words, how much is uncertainty about X REDUCED when we measure Y? That is the MI between the two variables. And it leads to one common formulation of MI:\n",
    "\n",
    "`MI = H(X) - H(X|Y)`\n",
    "\n",
    "where `H(X)` is the entropy of `X`, and `H(X|Y)` is the conditional entropy of `X` given that we've measured `Y` (it is the average entropy of `X` across all values of `Y`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed rnd num gen\n",
    "np.random.seed(0)\n",
    "\n",
    "# two discrete random arrays filled with 0's and 1's\n",
    "N = 100000   # number of data points\n",
    "x = np.round(np.random.rand(N))   # our x\n",
    "y = np.round(np.random.rand(N))   # our y\n",
    "\n",
    "# introduce some relationship bewteen x,y\n",
    "# quick hack will replace some percentage of \n",
    "# y with values in x to determine how related they are...\n",
    "# this is clunky, but quick :) \n",
    "p = .95     # how many values in x,y will be identical?\n",
    "ind = int(p*N)\n",
    "\n",
    "# replace existing values in y with values in x\n",
    "y[:ind] = x[:ind]\n",
    "\n",
    "# event probabilities for x\n",
    "px = np.zeros(2)\n",
    "px[0] = np.sum(x) / N   # probability that x==1\n",
    "px[1] = 1 - px[0]       # prob that x==0\n",
    "\n",
    "# entropy of x\n",
    "Hx = -np.sum( px * np.log2(px) )\n",
    "\n",
    "print(f'Entropy of X: {Hx}')\n",
    "\n",
    "# then compute average conditional entropy of x given y (Hxy).\n",
    "# 1) Compute the entropy of X given each possible value of Y\n",
    "# 2) Multiply H(X|Yi) with the probability of each Y (i.e. p(yi))\n",
    "# 3) Sum H(X|Yi) over all i\n",
    "\n",
    "# initialize Hxy\n",
    "Hxy = 0\n",
    "\n",
    "# loop over unique elements of y, in this case 0,1\n",
    "for i in np.arange(2): \n",
    "    \n",
    "    # probability that y==y(i) (prob of each y)\n",
    "    py = np.sum(y==i) / N\n",
    "\n",
    "    # then loop over all possible x's to compute entropy of x at each y\n",
    "    tmp=0\n",
    "    \n",
    "    for j in np.arange(2):\n",
    "        \n",
    "        px_y = np.sum( (x==j) & (y==i) ) / np.sum( y==i )    # e.g. prob x==1 when y==0\n",
    "        tmp += -( px_y * np.log2(px_y) )                     # entropy      \n",
    "        \n",
    "    # then tally up entropy of x given each specific y multiplied by the probability of that y (py) - this will \n",
    "    # give you the average conditional entropy...\n",
    "    Hxy += (py * tmp)\n",
    "\n",
    "# then we have everything we need to compute MI\n",
    "MI = Hx - Hxy\n",
    "\n",
    "print(f'Mutual Information between X,Y: {MI}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And then the more compact way using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "from math import isclose\n",
    "\n",
    "# sklearn implmentation (note that it uses natural log,\n",
    "# but we can convert to bits by multiplying by log2(e), where e is Euler's number\n",
    "#, which is the base for the natural log)\n",
    "sklean_MI = mutual_info_score(x,y) * np.log2(np.e)\n",
    "\n",
    "print(f'Mutual Information between X,Y: {sklean_MI}')\n",
    "\n",
    "# check our answer from above against sklearn...\n",
    "assert( isclose( MI,sklean_MI ) )\n",
    "print('whew...we did it right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets try a more complicated example where we have one discrete variable (like a categorical experimental condition) and one continuous variable (e.g. timeseries of activity)\n",
    "* This often comes up in experiments where you have a set of discrete stimuli and then a continuous output measure (e.g. LFP power, spike rate over a time window, clinical outcomes on a survey, housing prices, etc)\n",
    "    * For comparison, with our coin tosses, you can get the PDF by sum(heads)/total_tosses...\n",
    "    * But for a continuous metric where you have all unique values, this doesn't work (i.e. if each value happens once and only once then how do you compute the likelihood of observing that value?\n",
    "    * To solve this problem, we need to generate an estimate of the PDF based on our samples of the continuous data\n",
    "    * Common approaches are to sort the data into bins (histogram) to approximate the PDF, or to use Kernel Density Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up our variables - one discrete and one continuous\n",
    "* lets assume that our data came from an experiment where we have 4 different experimental conditions and we record some continuous DV in each condition (e.g. spike rates, bold response, stock market price, clinical outcome measure, ...whatever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of samples in our experiment\n",
    "N = 10000\n",
    "\n",
    "# Generate a list of labels for each condition\n",
    "num_conds = 4\n",
    "conds = np.repeat(np.arange(4), N/num_conds) # div by num_stims to keep output length constant\n",
    "\n",
    "# Now generate our list of continuous spike rates in response to each stimulus\n",
    "# Set it up so that there is a differential response to each of the 4 stims (i.e. MI>0)\n",
    "cond_means = [0,1,2,3] # mean of response in each condition\n",
    "\n",
    "# try these to increase MI\n",
    "# cond_means = [0,4,8,12] # mean of response in each condition\n",
    "# cond_means = [0,10,20,30] # mean of response in each condition\n",
    "# cond_means = [0,100,200,300] # mean of response in each condition\n",
    "\n",
    "cond_std = [1,1,1,1]   # std of response in each condition\n",
    "# cond_std = [.15,.15,.15,.15]   # std of response in each condition\n",
    "\n",
    "resp=[]\n",
    "for i in np.arange(num_conds):\n",
    "    resp = np.hstack((resp, ((np.random.randn(int(N/num_conds)) * cond_std[i]) + cond_means[i])))\n",
    "    \n",
    "plt.plot(np.arange(N), resp, 'k', linewidth=2)\n",
    "plt.xlabel('Trial Number')\n",
    "plt.ylabel('Resp(Hz)')\n",
    "plt.show()\n",
    "\n",
    "# also show as a histogram to get a feel for the distribution\n",
    "num_bins = 10 # change this! you'll get a feel for why using histograms for estimating PDFs is tricky\n",
    "plt.hist(resp, bins=num_bins)\n",
    "plt.xlabel('Resp(Hz)')\n",
    "plt.ylabel('Trial count')\n",
    "plt.show()\n",
    "\n",
    "# also show as a histogram for one condition...\n",
    "num_bins = 10 # change this! you'll get a feel for why using histograms for estimating PDFs is tricky\n",
    "plt.hist(resp[conds==0], bins=num_bins)\n",
    "plt.xlabel('Resp(Hz)')\n",
    "plt.ylabel('Trial count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now use binning to estimate a PDF for the continuous variable\n",
    "* Create N bins, sort the continuous data into the bins, then compute mutual information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# how many bins? \n",
    "nbins = 20\n",
    "\n",
    "# first bin the data using histogram. \n",
    "# however, just save the edges of each bin\n",
    "edges = np.histogram_bin_edges(resp, bins=nbins)\n",
    "\n",
    "# then sort all of the continuous responses in bins\n",
    "# use numpy digitize...you pass in the continuous \n",
    "# response and the bin edges and it will sort each \n",
    "# response into the appropriate bin\n",
    "bin_y = np.digitize( resp, edges )\n",
    "\n",
    "# then compute MI! the default unit is nats, so convert to bits \n",
    "# by multiplying by log2( e ) which is Eulerâ€™s number or the base of the natural log\n",
    "MI = mutual_info_score( conds, bin_y ) * np.log2( np.e )\n",
    "\n",
    "print(MI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How 'good' is the MI value? \n",
    "* To check, always good to think about the theoretical upper limit on mutual information\n",
    "* Remember the formula: `MI = Hx - Hxy`\n",
    "* So the max value of MI is Hx...if knowing y is totally diagnostic of x, then the entropy of x given y (Hxy) will be 0, and MI will equal Hx.\n",
    "* So you can figure out Hx, and then go back to the simulation above and either increase the difference between the means (cond_means), or decrease the std (cond_std). In either case, you will reduce overlap between the conditions and knowing y will become more predictive of the value of x (i.e. if you measure y, you'll be able to better guess what condition that data came from). As you do this, MI should approach Hx...\n",
    "* Another intuition: the more overlap between the response distributions in each condition, the lower MI (because knowing y will give you ambiguous information about the value of x). However, as the overlap goes down, MI will go up, and when there is no overlap, the MI == Hx. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how accurate are we? \n",
    "# first figure out Hx\n",
    "# easy cause there are 4 equally likely conditions\n",
    "print(f'Entropy of x (conds): {np.log2(4)}')\n",
    "print(f'Mutual Information: {MI}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized MI\n",
    "* Since the upper limit of MI is Hx, you can convert the unit of MI from bits to a proportion of total entropy (or put another way, as a proportion of the upper bound of information)\n",
    "* Normalized MI is NMI = MI/Hx, which has a theoretical upper limit of 1 and a lower limit of 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NMI = MI/np.log2(4)\n",
    "print('Normalized MI:', NMI)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
